{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne <br>\n",
    "Author Blog: **https://www.securitynik.com** <br>\n",
    "Author GitHub: **github.com/securitynik** <br>\n",
    "\n",
    "Author Books: [  <br>\n",
    "\n",
    "                \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\", \n",
    "                \n",
    "                \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\" \n",
    "            ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why this series?\n",
    "When teaching the SANS SEC595: Applied Data Science and Machine Learning for Cybersecurity Professionals \n",
    "**https://www.sans.org/cyber-security-courses/applied-data-science-machine-learning/** I am always asked,\n",
    "\"Will you be sharing your demo notebooks?\" or \"Can we get a copy of your demo notebooks?\" or ... well you get the point.\n",
    "My answer is always no. Not that I do not want to share, (sharing is caring :-D) , but the demo notebooks \n",
    "by themselves, would not make sense or add real value. Hence, this series! \n",
    "\n",
    "This is my supplemental work, similar to what I would do in the demos but with a lot more details and references.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Beginning Regularization - Early Stopping, Dropout, L2 (Ridge), L1 (Lasso)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The series includes the following: <br>\n",
    "01 - Beginning Numpy <br>\n",
    "02 - Beginning Tensorflow  <br>\n",
    "03 - Beginning PyTorch <br>\n",
    "04 - Beginning Pandas <br>\n",
    "05 - Beginning Matplotlib <br>\n",
    "06 - Beginning Data Scaling <br>\n",
    "07 - Beginning Principal Component Analysis (PCA) <br>\n",
    "08 - Beginning Machine Learning Anomaly Detection - Isolation Forest and Local Outlier Factor <br>\n",
    "09 - Beginning Unsupervised Machine Learning - Clustering - K-means and DBSCAN <br>\n",
    "10 - Beginning Supervise Learning - Machine Learning - Logistic Regression, Decision Trees and Metrics <br>\n",
    "11 - Beginning Linear Regression - Machine Learning <br>\n",
    "12 - Beginning Deep Learning - Anomaly Detection with AutoEncoders, Tensorflow <br>\n",
    "13 - Beginning Deep Learning - Anomaly Detection with AutoEncoders, PyTroch <br>\n",
    "14 - Beginning Deep Learning - Linear Regression, Tensorflow <br>\n",
    "15 - Beginning Deep Learning - Linear Regression, PyTorch <br>\n",
    "16 - Beginning Deep Learning - Classification, Tensorflow <br>\n",
    "17 - Beginning Deep Learning - Classification, Pytorch <br>\n",
    "18 - Beginning Deep Learning - Classification - regression - MIMO - Functional API Tensorflow <br> \n",
    "19 - Beginning Deep Learning - Convolution Networks - Tensorflow <br>\n",
    "20 - Beginning Deep Learning - Convolution Networks - PyTorch <br>\n",
    "21 - Beginning Regularization - Early Stopping, Dropout, L2 (Ridge), L1 (Lasso) <br>\n",
    "22 - Beginning Model TFServing <br>\n",
    "\n",
    "But conn.log is not the only log file within Zeek. Let's build some models for DNS and HTTP logs. <br>\n",
    "I choose unsupervised, because there are no labels coming with these data. <br>\n",
    "\n",
    "23 - Continuing Anomaly Learning - Zeek DNS Log - Machine Learning <br>\n",
    "24 - Continuing Unsupervised Learning - Zeek HTTP Log - Machine Learning <br>\n",
    "\n",
    "This was a specific ask by someone in one of my class. <br>\n",
    "25 - Beginning - Reading Executables and Building a Neural Network to make predictions on suspicious vs suspicious  <br><br>\n",
    "\n",
    "With 25 notebooks in this series, it is quite possible there are things I could have or should have done differently.  <br>\n",
    "If you find any thing, you think fits those criteria, drop me a line. <br>\n",
    "\n",
    "If you find this series beneficial, I would greatly appreciate your feedback.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is used to address overfitting. Overfitting is where the model learns very well on the training data, but when it's time to make prediction, the model is terrible. More politely, the model fails to generalize. Think of a model that has 99% accuracy during training, as we saw in some of our imbalance class models. However, when provided with test or previously unseen data, the model operates under 50%. That would be a major case of overfitting.\n",
    "\n",
    "There are a number of strategies to address overfitting. \n",
    "One can start by reducing the number of features through manual or automatic processes such as dimensionality reduction mechanisms such as PCA as was explained in the PCA notebook: \n",
    "    07 - Beginning Principal Component Analysis (PCA)\n",
    "\n",
    "You can also also try to get more data.\n",
    "Then again, if you could have gotten that data, you would have more than likely used it as part of the initial training ;-)\n",
    "\n",
    "With regularization, we want to keep all the features but reduce the magnitude of the parameters (weights and biases)\n",
    "In this case you want to penalize the cost function by adding a regularization term. There is also a regularization parameter Lambda which is also used. Lambda controls the trade off between fitting the training data and keeping the parameters small enough\n",
    "\n",
    "By adding the regularization term to the cost function we are able to shrink all the parameters. Realistically, we want to penalize large weights as the bias is not normally regularized\n",
    "\n",
    "Lambda is another hyperparameter that will have to be tuned.\n",
    "\n",
    "When Lambda is 0, you are basically not using the regularization term. Thus the model will continue to overfit.\n",
    "The larger your Lambda, the closer your weights need to be to 0. This also means your model will underfit\n",
    "Hence the idea is to find the right value for Lambda. One that is not too large or too small. One that is just right\n",
    "\n",
    "Regularization = Loss Function + Penalty Term\n",
    "\n",
    "There common types of regularization.\n",
    "    1.  L2 - Ridge Regression\n",
    "        - Attempts to keep the weights close to 0 but not 0\n",
    "        - All parameters are penalized\n",
    "    2.  L1  - Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "        - Penalized some features\n",
    "        - It has automatic feature selection capabilities\n",
    "    3.  ElasticNet\n",
    "        - Combination of L1 and L2\n",
    "    3.  Dropout\n",
    "        - Used in deep learning\n",
    "\n",
    "Reference: <br>\n",
    "https://www.youtube.com/watch?v=u73PU6Qwl1I <br>\n",
    "https://www.youtube.com/watch?v=KvtGD37Rm5I <br>\n",
    "https://www.youtube.com/watch?v=iuJgyiS7BKM <br>\n",
    "https://www.youtube.com/watch?v=ce4CPW8AFE4 <br>\n",
    "https://www.youtube.com/watch?v=SCj3h47dKL0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.053276</td>\n",
       "      <td>208</td>\n",
       "      <td>976</td>\n",
       "      <td>6</td>\n",
       "      <td>528</td>\n",
       "      <td>6</td>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.383646</td>\n",
       "      <td>227</td>\n",
       "      <td>692369</td>\n",
       "      <td>202</td>\n",
       "      <td>10739</td>\n",
       "      <td>117</td>\n",
       "      <td>240893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430183</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430184</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430185</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430186</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430187</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4236125 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id.resp_p  duration  orig_bytes  resp_bytes  orig_pkts   \n",
       "1            48798  0.000000           0           0          0  \\\n",
       "2            48804  0.000000           0           0          0   \n",
       "4            27761  0.000000           0           0          0   \n",
       "5             8888  0.053276         208         976          6   \n",
       "6             8888  0.383646         227      692369        202   \n",
       "...            ...       ...         ...         ...        ...   \n",
       "4430183       9200  0.000051           0           0          1   \n",
       "4430184       9200  0.000023           0           0          0   \n",
       "4430185       9200  0.000005           0           0          0   \n",
       "4430186       9200  0.000005           0           0          0   \n",
       "4430187       9200  0.000005           0           0          0   \n",
       "\n",
       "         orig_ip_bytes  resp_pkts  resp_ip_bytes  label  \n",
       "1                    0          0              0      0  \n",
       "2                    0          0              0      0  \n",
       "4                    0          0              0      0  \n",
       "5                  528          6           1296      0  \n",
       "6                10739        117         240893      0  \n",
       "...                ...        ...            ...    ...  \n",
       "4430183             52          1             40      0  \n",
       "4430184              0          1             40      0  \n",
       "4430185              0          1             40      0  \n",
       "4430186              0          1             40      0  \n",
       "4430187              0          1             40      0  \n",
       "\n",
       "[4236125 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the notebooks on Pandas, Matplotlib and Scaling\n",
    "#   04 - Beginning Pandas <br>\n",
    "#   05 - Beginning Matplotlib <br>\n",
    "#   06 - Beginning Data Scaling <br>\n",
    "# we loaded our dataset such as\n",
    "df_conn = pd.read_csv(r'df_conn_with_labels.csv', index_col=0)\n",
    "df_conn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file represents Zeek (formerly Bro) connection log - conn.log`. \n",
    "Zeek is a framework used for Network Security Monitoring. \n",
    "This entire series is based on using Zeek's data. \n",
    "The majority of the notebooks use the conn.log\n",
    "You can learn more about Zeek here:\n",
    "   \n",
    "    https://zeek.org/\n",
    "\n",
    "Alternatively, come hang out with us in the:\n",
    "SANS SEC595: Applied Data Science and Machine Learning for Cybersecurity Professionals\n",
    "\n",
    "        https://www.sans.org/cyber-security-courses/applied-data-science-machine-learning/ OR\n",
    "\n",
    "SEC503 SEC503: Network Monitoring and Threat Detection In-Depth\n",
    "\n",
    "        https://www.sans.org/cyber-security-courses/network-monitoring-threat-detection/\n",
    "\n",
    "\n",
    "Here are also some blog posts on using Zeek for security monitoring\n",
    "Installing Zeek: \n",
    "\n",
    "        https://www.securitynik.com/2020/06/installing-zeek-314-on-ubuntu-2004.html\n",
    "\n",
    "Detecting PowerShell Empire Usage: \n",
    "\n",
    "        https://www.securitynik.com/2022/02/powershell-empire-detection-with-zeek.html\n",
    "\n",
    "Detecting Log4J Vulnerability Exploitation: \n",
    "\n",
    "        https://www.securitynik.com/2021/12/continuing-log4shell-zeek-detection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.053276</td>\n",
       "      <td>208</td>\n",
       "      <td>976</td>\n",
       "      <td>6</td>\n",
       "      <td>528</td>\n",
       "      <td>6</td>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.383646</td>\n",
       "      <td>227</td>\n",
       "      <td>692369</td>\n",
       "      <td>202</td>\n",
       "      <td>10739</td>\n",
       "      <td>117</td>\n",
       "      <td>240893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.054294</td>\n",
       "      <td>208</td>\n",
       "      <td>977</td>\n",
       "      <td>6</td>\n",
       "      <td>528</td>\n",
       "      <td>6</td>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.424980</td>\n",
       "      <td>227</td>\n",
       "      <td>791633</td>\n",
       "      <td>176</td>\n",
       "      <td>9387</td>\n",
       "      <td>174</td>\n",
       "      <td>367737</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8888</td>\n",
       "      <td>0.272567</td>\n",
       "      <td>227</td>\n",
       "      <td>406607</td>\n",
       "      <td>171</td>\n",
       "      <td>9127</td>\n",
       "      <td>71</td>\n",
       "      <td>169939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430183</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430184</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430185</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430186</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430187</th>\n",
       "      <td>9200</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207608 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id.resp_p  duration  orig_bytes  resp_bytes  orig_pkts   \n",
       "5             8888  0.053276         208         976          6  \\\n",
       "6             8888  0.383646         227      692369        202   \n",
       "7             8888  0.054294         208         977          6   \n",
       "8             8888  0.424980         227      791633        176   \n",
       "9             8888  0.272567         227      406607        171   \n",
       "...            ...       ...         ...         ...        ...   \n",
       "4430183       9200  0.000051           0           0          1   \n",
       "4430184       9200  0.000023           0           0          0   \n",
       "4430185       9200  0.000005           0           0          0   \n",
       "4430186       9200  0.000005           0           0          0   \n",
       "4430187       9200  0.000005           0           0          0   \n",
       "\n",
       "         orig_ip_bytes  resp_pkts  resp_ip_bytes  label  \n",
       "5                  528          6           1296      0  \n",
       "6                10739        117         240893      0  \n",
       "7                  528          6           1297      0  \n",
       "8                 9387        174         367737      0  \n",
       "9                 9127         71         169939      0  \n",
       "...                ...        ...            ...    ...  \n",
       "4430183             52          1             40      0  \n",
       "4430184              0          1             40      0  \n",
       "4430185              0          1             40      0  \n",
       "4430186              0          1             40      0  \n",
       "4430187              0          1             40      0  \n",
       "\n",
       "[207608 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at above, we see a number of records with 0s. \n",
    "# These will add no value to our learning process\n",
    "# Let's find all those records and drop them\n",
    "# Reference: https://stackoverflow.com/questions/13851535/how-to-delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression\n",
    "df_conn = df_conn.drop(df_conn[(df_conn.duration == 0 ) & (df_conn.orig_bytes == 0 ) \n",
    "                               & (df_conn.resp_bytes == 0 ) & (df_conn.orig_pkts == 0 )  \\\n",
    "                                & (df_conn.orig_ip_bytes == 0 ) & (df_conn.resp_pkts == 0 ) \\\n",
    "                                    & (df_conn.resp_ip_bytes == 0 )].index)\n",
    "df_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5            1296\n",
       "6          240893\n",
       "7            1297\n",
       "8          367737\n",
       "9          169939\n",
       "            ...  \n",
       "4430183        40\n",
       "4430184        40\n",
       "4430185        40\n",
       "4430186        40\n",
       "4430187        40\n",
       "Name: resp_ip_bytes, Length: 207608, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the responder IP Bytes as the label\n",
    "y_labels = df_conn.resp_ip_bytes\n",
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.88800e+03, 5.32760e-02, 2.08000e+02, ..., 6.00000e+00,\n",
       "        5.28000e+02, 6.00000e+00],\n",
       "       [8.88800e+03, 3.83646e-01, 2.27000e+02, ..., 2.02000e+02,\n",
       "        1.07390e+04, 1.17000e+02],\n",
       "       [8.88800e+03, 5.42940e-02, 2.08000e+02, ..., 6.00000e+00,\n",
       "        5.28000e+02, 6.00000e+00],\n",
       "       ...,\n",
       "       [9.20000e+03, 5.00000e-06, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [9.20000e+03, 5.00000e-06, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00],\n",
       "       [9.20000e+03, 5.00000e-06, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop two features, labels and resp_ip_bytes\n",
    "# Put the data into a numpy array\n",
    "X_data = df_conn.drop(columns=['label', 'resp_ip_bytes'], inplace=False).values\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207608, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape fo the X_data\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'duration')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot of originator vs responder bytes\n",
    "# Using matplotlib was covered in notebook:\n",
    "#   05 - Beginning Matplotlib\n",
    "plt.title('Responder Port vs Duration')\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1])\n",
    "plt.xlabel('resp port')\n",
    "plt.ylabel('duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library to split the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166086, 7), (166086,), (41522, 7), (41522,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_labels, test_size=0.2, train_size=0.8, random_state=10)\n",
    "X_train.shape,  y_train.shape, X_test.shape,  y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the scaler\n",
    "# Scaling was discussed in notebook:\n",
    "#   06 - Beginning Data Scaling\n",
    "min_max = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "X_train = min_max.transform(X_train)\n",
    "X_test = min_max.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.00629585e+05, -8.22793749e+06, -8.16939661e+05,  1.21688692e+06,\n",
       "        2.12315659e+08, -4.97888699e+08,  7.17674612e+08])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove scientific notations\n",
    "# np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Build a linear regression model without any type of \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "# Linear Regression was covered in notebooks:\n",
    "#   11. Beginning Linear Regression - Machine Learning\n",
    "#   14. Beginning Deep Learning, - Linear Regression, Tensorflow\n",
    "#   15. Beginning Deep Learning, - Linear Regression, PyTorch\n",
    "lr = LinearRegression(fit_intercept=True, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "# Get the weights\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-794.7710600432765"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the bias\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9847879961994693, 0.8878412502495496)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the score on the train and test sets\n",
    "#   https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "# The score on the the training data is very good. The closer to 1 the better off we are\n",
    "# Looking at test data, we see this value is negative, meaning the model is terrible\n",
    "# This is obviously a clear case of overfitting\n",
    "lr.score(X_train, y_train), lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ridge and Lasso regularization\n",
    "from sklearn.linear_model import Ridge, Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9847851556050645, 0.8889768065073369)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leveraging Ridge regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
    "# Fit the model\n",
    "# Alpha controls the regularization strength and must be non negative\n",
    "# When using other solvers, the scores was way terrible.\n",
    "# With svd, the score between the model without regularization was not much different\n",
    "ridge = Ridge(alpha=0.001, solver='svd').fit(X_train, y_train)\n",
    "\n",
    "# Get the scores of both the train and test set scores\n",
    "ridge.score(X_train, y_train), ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.02776000e+05, -8.19514200e+06, -9.40709000e+05,  1.27805800e+06,\n",
       "        2.07443368e+08, -4.93085744e+08,  7.18347961e+08])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the ridge coefficients\n",
    "np.round(a=ridge.coef_, decimals=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-660.1465566982952"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the bias\n",
    "ridge.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9847879851967936, 0.8879182058465387)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leveraging Lasso\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "# Fit the model\n",
    "# Alpha controls the regularization strength and must be non negative\n",
    "lasso = Lasso(alpha=.1, selection='random').fit(X_train, y_train)\n",
    "\n",
    "# Get the scores of both the train and test set scores\n",
    "# Lasso\n",
    "lasso.score(X_train, y_train), lasso.score(X_test, y_test), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.00761921e+05, -8.22253301e+06, -8.06046704e+05,  1.21627654e+06,\n",
       "        2.11941338e+08, -4.97577491e+08,  7.17769933e+08])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Lasso coefficients\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-787.0344869213877"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above, it seems neither Ridge nor Lasso helped this model addressed the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5191/5191 [==============================] - 10s 2ms/step - loss: 6133520007168.0000 - mse: 6130970918912.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2370694d6c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a linear regression model with Tensorflow \n",
    "model = tf.keras.Sequential(name='regression_model')\n",
    "model.add(tf.keras.layers.Input(shape=(X_train.shape[1])))\n",
    "# Add L2 regularization\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='linear', kernel_regularizer='l2'))\n",
    "#model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x=X_train, y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(7, 1) dtype=float32, numpy=\n",
       " array([[201561.16 ],\n",
       "        [ 29028.03 ],\n",
       "        [ 40922.535],\n",
       "        [207242.83 ],\n",
       "        [362990.88 ],\n",
       "        [199107.23 ],\n",
       "        [600808.75 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([45894.117], dtype=float32)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model weights\n",
    "# There was no noticeable difference when setting the regularization to \"l1\" or \"l2\"\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5191/5191 [==============================] - 9s 2ms/step - loss: 6125646249984.0000 - mse: 6119460175872.0000\n",
      "1298/1298 [==============================] - 2s 2ms/step - loss: 2851630219264.0000 - mse: 2845445193728.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6119460175872.0, 2845445193728.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model against the train and test data\n",
    "_, mse_train = model.evaluate(X_train, y_train)\n",
    "_, mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "mse_train, mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.679793525825844e+24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much does the two models vary\n",
    "np.var([mse_train, mse_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the coefficients for both L2 and L1 from both the perspective of machine and deep learning,\n",
    "we see that for L1 none of the coefficients are 0. \n",
    "The expectation was that some of the coefficients would be 0 for L1 and for L2, they would be close to 0\n",
    "This is not the case for this dataset. \n",
    "Then again, when we look at the machine learning models, we see there is basically no overfitting for the default linear regression model.\n",
    "\n",
    "Brining in the Boson Housing dataset, which has 105 features vs the 7 our dataset has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.067815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.577505</td>\n",
       "      <td>0.641607</td>\n",
       "      <td>0.269203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059749</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.018655</td>\n",
       "      <td>0.082503</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089680</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.547998</td>\n",
       "      <td>0.782698</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058064</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.306021</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.113111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204470</td>\n",
       "      <td>0.041808</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.694386</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058064</td>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.306021</td>\n",
       "      <td>0.547514</td>\n",
       "      <td>0.035109</td>\n",
       "      <td>0.979580</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.441813</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.066412</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.421118</td>\n",
       "      <td>0.645222</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.988585</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.421118</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>0.681771</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.162090</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.882553</td>\n",
       "      <td>0.195787</td>\n",
       "      <td>0.975392</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.048003</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.490324</td>\n",
       "      <td>0.760041</td>\n",
       "      <td>0.105293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.181239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202815</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.654340</td>\n",
       "      <td>0.907312</td>\n",
       "      <td>0.094381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.096414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107892</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.889804</td>\n",
       "      <td>0.114514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.162694</td>\n",
       "      <td>0.021512</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.885843</td>\n",
       "      <td>0.117127</td>\n",
       "      <td>0.982677</td>\n",
       "      <td>0.129930</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.473079</td>\n",
       "      <td>0.802266</td>\n",
       "      <td>0.125072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.027852</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.151649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169702</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1         2    3         4         5         6         7     \n",
       "0    0.000000  0.18  0.067815  0.0  0.314815  0.577505  0.641607  0.269203  \\\n",
       "1    0.000236  0.00  0.242302  0.0  0.172840  0.547998  0.782698  0.348962   \n",
       "2    0.000236  0.00  0.242302  0.0  0.172840  0.694386  0.599382  0.348962   \n",
       "3    0.000293  0.00  0.063050  0.0  0.150206  0.658555  0.441813  0.448545   \n",
       "4    0.000705  0.00  0.063050  0.0  0.150206  0.687105  0.528321  0.448545   \n",
       "..        ...   ...       ...  ...       ...       ...       ...       ...   \n",
       "501  0.000633  0.00  0.420455  0.0  0.386831  0.580954  0.681771  0.122671   \n",
       "502  0.000438  0.00  0.420455  0.0  0.386831  0.490324  0.760041  0.105293   \n",
       "503  0.000612  0.00  0.420455  0.0  0.386831  0.654340  0.907312  0.094381   \n",
       "504  0.001161  0.00  0.420455  0.0  0.386831  0.619467  0.889804  0.114514   \n",
       "505  0.000462  0.00  0.420455  0.0  0.386831  0.473079  0.802266  0.125072   \n",
       "\n",
       "          8         9    ...       95        96        97        98    \n",
       "0    0.000000  0.208015  ...  0.059749  0.208015  0.018655  0.082503  \\\n",
       "1    0.043478  0.104962  ...  0.058064  0.104962  0.021462  0.306021   \n",
       "2    0.043478  0.104962  ...  0.058064  0.103885  0.006661  0.306021   \n",
       "3    0.086957  0.066794  ...  0.043345  0.066412  0.002230  0.421118   \n",
       "4    0.086957  0.066794  ...  0.043345  0.066794  0.006635  0.421118   \n",
       "..        ...       ...  ...       ...       ...       ...       ...   \n",
       "501  0.000000  0.164122  ...  0.146662  0.162090  0.035958  0.798551   \n",
       "502  0.000000  0.164122  ...  0.146662  0.164122  0.033286  0.798551   \n",
       "503  0.000000  0.164122  ...  0.146662  0.164122  0.017707  0.798551   \n",
       "504  0.000000  0.164122  ...  0.146662  0.162694  0.021512  0.798551   \n",
       "505  0.000000  0.164122  ...  0.146662  0.164122  0.027852  0.798551   \n",
       "\n",
       "          99        100       101       102       103   104  \n",
       "0    0.287234  0.025759  1.000000  0.089680  0.008042  24.0  \n",
       "1    0.553191  0.113111  1.000000  0.204470  0.041808  21.6  \n",
       "2    0.547514  0.035109  0.979580  0.062814  0.004028  34.7  \n",
       "3    0.645222  0.021667  0.988585  0.033197  0.001115  33.4  \n",
       "4    0.648936  0.064464  1.000000  0.099338  0.009868  36.2  \n",
       "..        ...       ...       ...       ...       ...   ...  \n",
       "501  0.882553  0.195787  0.975392  0.216382  0.048003  22.4  \n",
       "502  0.893617  0.181239  1.000000  0.202815  0.041134  20.6  \n",
       "503  0.893617  0.096414  1.000000  0.107892  0.011641  23.9  \n",
       "504  0.885843  0.117127  0.982677  0.129930  0.017180  22.0  \n",
       "505  0.893617  0.151649  1.000000  0.169702  0.028799  11.9  \n",
       "\n",
       "[506 rows x 105 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Boston Housing dataset from GitHub\n",
    "df_boston = pd.read_csv('https://raw.githubusercontent.com/m-mehdi/tutorials/main/boston_housing.csv', header=None)\n",
    "df_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 105)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can already see the shape of 506 rows and 105 columns above\n",
    "# Let's confirm the shape\n",
    "df_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 104), (506,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the X_Data by excluding the last column\n",
    "# The last column is used as the label\n",
    "# This means we now have 104 rather than 105 features\n",
    "X_boston = df_boston.values[:, :-1]\n",
    "\n",
    "# Setup the y_labels by taking the last column\n",
    "y_boston = df_boston.values[:, -1]\n",
    "\n",
    "# Confirming the shape of the dataset\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 104), (404,), (102, 104), (102,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = \\\n",
    "    train_test_split(X_boston, y_boston, test_size=0.2, train_size=0.8, random_state=10)\n",
    "X_train_boston.shape,  y_train_boston.shape, X_test_boston.shape,  y_test_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the scaler\n",
    "#   Scaling was covered in notebook\n",
    "#   06 - Beginning Data Scaling\n",
    "min_max = MinMaxScaler(feature_range=(0,1)).fit(X_train_boston)\n",
    "X_train_boston = min_max.transform(X_train_boston)\n",
    "X_test_boston = min_max.transform(X_test_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-717.,  -36.,  -83.,    4.,   17.,   52.,   27.,  -33.,   25.,\n",
       "         40.,    4.,   30.,   19.,    4.,    7.,  942.,   14.,   -7.,\n",
       "         47.,  -20.,   -7.,  114., -771.,  385.,  -17.,   35.,   -9.,\n",
       "         -2.,   -2.,   -9.,    7.,    0.,   -6.,   -1.,   18.,  -12.,\n",
       "         55.,   -9.,   39.,   -2.,   -2.,   27.,    6.,   12.,  -18.,\n",
       "         -1.,   -0.,   22.,   -1.,    4.,  -12.,  -30.,    7.,    6.,\n",
       "         -4.,    4.,   -5.,   12.,   -6.,    0.,   11.,  -29.,   13.,\n",
       "        -10.,   21.,  -33.,   -5.,   19.,    9.,   -5.,   21.,   15.,\n",
       "        -75.,  -22.,   -4.,  -17.,    3.,   -6.,   22.,  -10.,    5.,\n",
       "        -26.,  -30.,   45.,  -10.,  -19.,    1.,  -40.,   20.,  -55.,\n",
       "         86.,  -13.,  -10.,  -19.,  -30.,   29.,   -5.,  -21.,   -6.,\n",
       "         20.,   -4.,   -7.,   -7.,   18.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a linear regression model without any regularization\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "lr = LinearRegression(fit_intercept=True, n_jobs=-1).fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Get the weights\n",
    "np.round(lr.coef_,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.499137579226119"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get th eintercept\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9319230461477529, 0.8749973786104315)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the score on the train and test sets\n",
    "# There results shows overfitting\n",
    "# On the training data the score is 93% on the test daa it is 87%.\n",
    "# This problem seems like one that regularization may play a role\n",
    "lr.score(X_train_boston, y_train_boston), lr.score(X_test_boston, y_test_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8688451090297894, 0.8448055172494531)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leveraging Ridge regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
    "# Fit the model\n",
    "# Alpha controls the regularization strength and must be non-negative\n",
    "# When using other solvers, the scores was way terrible.\n",
    "# With svd, the score between the model without regularization was not much different\n",
    "# With an alpha of 0.9, we have now significantly reduced the overfitting\n",
    "ridge = Ridge(alpha=0.9, solver='svd').fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Get the scores of both the train and test set scores\n",
    "ridge.score(X_train_boston, y_train_boston), ridge.score(X_test_boston, y_test_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  1.,  0.,  1.,  9.,  1., -4.,  4., -0., -1.,  2., -1.,\n",
       "        1.,  1., -1.,  6., -1., -1., -1., -2., -2., -1., -1., -2., -1.,\n",
       "        2., -2.,  1.,  1.,  5., -2., -2.,  0.,  2., -0., -0., -2.,  1.,\n",
       "        1.,  1., -3.,  1., -4.,  2.,  2., -1.,  2., -3.,  0., -5., -4.,\n",
       "        2., -0., -1.,  0.,  1.,  3., -2., -1., -2., -2., -1., -2., -2.,\n",
       "       -3., -1., -0., 15.,  3.,  2., -5., -7., -5.,  9., -9.,  0., -2.,\n",
       "        4.,  0., -1., -3., -3.,  1., -0., -5.,  1., -4.,  5., -0.,  1.,\n",
       "        3.,  2., -8., -0.,  4., -1., -7.,  1., -1., -2., -0., -4.,  9.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Ridge coefficients\n",
    "# As expected, when compared to the model without regularization, the weights are closer to 0 than above\n",
    "np.round(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.142589827650163"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the bias\n",
    "ridge.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7706817030794093, 0.7625729612691312)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leveraging Lasso\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "# Fit the model\n",
    "# Alpha controls the regularization strength and must be non negative\n",
    "\n",
    "# with alpha of .9 the model did terrible\n",
    "#lasso = Lasso(alpha=.9, selection='random').fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=.1, selection='random').fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# Get the scores of both the train and test set scores\n",
    "# Lasso\n",
    "lasso.score(X_train_boston, y_train_boston), lasso.score(X_test_boston, y_test_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.,   0.,  -0.,   0.,  -0.,   0.,   0.,  -0.,   0.,  -0.,  -4.,\n",
       "         0.,  -0.,  -0.,   0.,  -0.,   0.,  -0.,  -0.,  -0.,  -0.,  -0.,\n",
       "        -0.,  -0.,  -0.,  -0.,   0.,  -0.,   0.,   0.,   0.,   0.,  -0.,\n",
       "         0.,  -0.,  -0.,   0.,  -0.,  -0.,   0.,  -0.,  -0.,  -0.,  -0.,\n",
       "         0.,  -0.,  -0.,  -0.,  -0.,   0.,  -0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   1.,   0.,  -0.,  -0.,  -0.,  -1.,  -0.,  -0.,  -0.,\n",
       "        -0.,  -0.,  23.,   0.,  -0.,  -0.,  -0.,  -0.,   5., -17.,   0.,\n",
       "        -0.,   0.,  -0.,  -0.,   0.,  -0.,  -0.,  -0.,  -2.,  -0.,  -0.,\n",
       "        -0.,   0.,   0.,   0.,   0.,  -0.,  -0.,  -0.,  -0.,  -3.,  -0.,\n",
       "        -0.,  -0.,   0.,  -0.,  -0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the LASSO coefficients\n",
    "# As can be seen here, a significant number of the parameters have been pushed to 0\n",
    "np.round(lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 9)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More specifically, 95 of the 104 features have been nullified.\n",
    "# This means that 9 features are considered useful in this regularized lasso model with 104 features\n",
    "# There are 104 features because I took the last one as the label\n",
    "sum(lasso.coef_ == 0), sum(lasso.coef_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before leveraging Dropout, let's figure out what is being done with dropout\n",
    "# Dropout has to be between 0 and 1\n",
    "# The formula is basically 1 - dropout rate\n",
    "# The result is what will be retained.\n",
    "# Hence a dropout rate of 0.2 means we have 1-.2 = .8. Hence 20% will be made 0\n",
    "\n",
    "# Setup an array of 10 items\n",
    "x = np.array([[1,2,3,4,5], [6, 7, 8,9,10]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/(1-0.2)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2500,  2.5000,  3.7500,  5.0000,  0.0000],\n",
       "        [ 0.0000,  8.7500, 10.0000, 11.2500, 12.5000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using PyTorch dropout\n",
    "# With a dropout of 0.2, we drop 20% of the 10 items\n",
    "# This means 2 or 20% of the items will be made 0 as seen below\n",
    "#   https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "# You might also be wondering why the number change from their original value\n",
    "# That's because each non 0 value has been scaled by (1 / 1-dropout_rate)\n",
    "# hence 1 has now become 1 -> (1/(1-0.2)) * 1 = 1.25\n",
    "# Similarly, 2 has become (1/(1-0.2)) * 2 = 1.5\n",
    "torch.manual_seed(11)\n",
    "torch.nn.Dropout(p=0.2, inplace=True)(torch.tensor(data=x, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float64, numpy=\n",
       "array([[ 1.25,  0.  ,  3.75,  5.  ,  0.  ],\n",
       "       [ 7.5 ,  8.75, 10.  , 11.25, 12.5 ]])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout with Tensorflow\n",
    "#   https://www.tensorflow.org/api_docs/python/tf/nn/dropout\n",
    "tf.nn.dropout(x=x.astype(float), rate=.2, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 1s 5ms/step - loss: 167.7169 - mse: 167.7169\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 61.5843 - mse: 61.5843\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 83.0649 - mse: 83.0649\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.6415 - mse: 69.6415\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 61.1927 - mse: 61.1927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23706459cc0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With that understanding ...\n",
    "# Build a linear regression model with Tensorflow without dropout\n",
    "\n",
    "# Set the random seed\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "\n",
    "model = tf.keras.Sequential(name='regression_model')\n",
    "model.add(tf.keras.layers.Input(shape=(X_train_boston.shape[1]), name='input_layer'))\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='relu', name='hidden_layer'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x=X_train_boston, y=y_train_boston, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 4ms/step - loss: 85.8886 - mse: 85.8886\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 80.6470 - mse: 80.6470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(85.88861083984375, 80.64696502685547)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model against the train and test data\n",
    "_, mse_train_boston = model.evaluate(X_train_boston, y_train_boston)\n",
    "_, mse_test_boston = model.evaluate(X_test_boston, y_test_boston)\n",
    "\n",
    "mse_train_boston, mse_test_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.868712707204395"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the variance between the train and test loss\n",
    "np.var([mse_train_boston, mse_test_boston])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 1s 3ms/step - loss: 138.1784 - mse: 138.1784\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 117.2364 - mse: 117.2364\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 86.7377 - mse: 86.7377\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 84.1899 - mse: 84.1899\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 51.5634 - mse: 51.5634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23709653c70>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "\n",
    "# Build a linear regression model with Tensorflow using dropout\n",
    "model = tf.keras.Sequential(name='regression_model')\n",
    "model.add(tf.keras.layers.Input(shape=(X_train_boston.shape[1]), name='input_layer'))\n",
    "model.add(tf.keras.layers.Dense(units=256, activation='relu', name='hidden_layer'))\n",
    "\n",
    "# Dropout 50% of the neurons\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5, name='dropout'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x=X_train_boston, y=y_train_boston, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 79.1499 - mse: 79.1499\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 74.8772 - mse: 74.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79.14987182617188, 74.87723541259766)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model against the train and test data\n",
    "_, mse_train_boston = model.evaluate(X_train_boston, y_train_boston)\n",
    "_, mse_test_boston = model.evaluate(X_test_boston, y_test_boston)\n",
    "\n",
    "# A Dropout rate of 0.5 seems to be a reasonable match for this problem at this time\n",
    "mse_train_boston, mse_test_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.563855480650091"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the variance between the two models\n",
    "# The variance here is much smaller than it was without dropout\n",
    "np.var([mse_train_boston, mse_test_boston])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a PyTorch model without dropout\n",
    "torch_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=X_train_boston.shape[1], out_features=256),\n",
    "    torch.nn.Linear(in_features=256, out_features=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.2627e-04, 2.1000e-01, 1.8988e-01, 0.0000e+00, 1.1111e-01, 4.6024e-01,\n",
       "          4.4078e-01, 5.1665e-01, 1.3043e-01, 1.0687e-01, 4.4681e-01, 9.9662e-01,\n",
       "          3.1983e-01, 8.5797e-07, 1.0566e-01, 2.7200e-04, 0.0000e+00, 1.7489e-04,\n",
       "          6.5304e-04, 4.4544e-04, 9.4679e-03, 1.2082e-04, 1.0829e-04, 5.1189e-04,\n",
       "          9.2314e-04, 6.6925e-04, 4.4100e-02, 2.6646e-01, 0.0000e+00, 2.0826e-01,\n",
       "          1.1871e-01, 2.3904e-01, 1.2063e-01, 1.5750e-01, 5.5603e-02, 1.1729e-01,\n",
       "          2.1142e-01, 2.5108e-01, 3.6055e-02, 0.0000e+00, 3.0102e-02, 1.3515e-01,\n",
       "          8.4744e-02, 4.2152e-01, 3.8302e-02, 2.0293e-02, 1.0633e-01, 1.8924e-01,\n",
       "          7.9622e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2346e-02,\n",
       "          7.4634e-02, 4.8976e-02, 4.0432e-01, 1.8295e-02, 1.6398e-02, 7.7512e-02,\n",
       "          1.1074e-01, 4.6843e-02, 2.1182e-01, 2.4623e-01, 3.9210e-01, 6.0031e-02,\n",
       "          5.3807e-02, 2.5434e-01, 5.1352e-01, 3.5866e-01, 1.9429e-01, 4.6045e-01,\n",
       "          5.7493e-02, 4.7696e-02, 2.1527e-01, 4.3929e-01, 1.4255e-01, 2.6726e-01,\n",
       "          2.4978e-01, 1.4835e-01, 2.6863e-01, 5.3191e-01, 4.7675e-01, 1.7013e-02,\n",
       "          1.5249e-02, 7.2082e-02, 1.2999e-01, 4.2183e-02, 1.1421e-02, 5.9847e-02,\n",
       "          1.0651e-01, 3.7809e-02, 1.9964e-01, 4.6171e-01, 1.7514e-01, 9.9325e-01,\n",
       "          3.2231e-01, 1.0456e-01]]),\n",
       " tensor([[5.4479e-04, 3.3000e-01, 6.3050e-02, 0.0000e+00, 1.7901e-01, 5.8536e-01,\n",
       "          5.6849e-01, 2.0319e-01, 2.6087e-01, 6.6794e-02, 6.1702e-01, 9.9107e-01,\n",
       "          1.9445e-01, 2.9680e-07, 9.7657e-02, 5.3120e-05, 0.0000e+00, 1.6572e-04,\n",
       "          4.8851e-04, 3.3789e-04, 2.1945e-03, 1.4212e-04, 3.9807e-05, 4.1576e-04,\n",
       "          5.3993e-04, 2.4182e-04, 1.0890e-01, 1.3904e-01, 0.0000e+00, 5.2727e-01,\n",
       "          2.3725e-01, 4.8447e-01, 7.4701e-02, 4.9500e-01, 5.4610e-02, 2.5452e-01,\n",
       "          3.3039e-01, 2.4239e-01, 3.9753e-03, 0.0000e+00, 1.6104e-02, 5.7076e-02,\n",
       "          3.6291e-02, 5.5158e-02, 2.5436e-02, 4.2113e-03, 4.8759e-02, 6.2487e-02,\n",
       "          1.6242e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2045e-02,\n",
       "          1.5293e-01, 1.0177e-01, 2.5671e-01, 5.8950e-02, 1.6512e-02, 1.7245e-01,\n",
       "          1.7741e-01, 4.6363e-02, 3.4265e-01, 4.0390e-01, 1.9653e-01, 1.5270e-01,\n",
       "          4.2772e-02, 4.4672e-01, 6.4949e-01, 2.8024e-01, 3.2318e-01, 2.3403e-01,\n",
       "          1.4830e-01, 3.8447e-02, 3.8340e-01, 5.6341e-01, 1.1294e-01, 4.1506e-02,\n",
       "          1.9687e-01, 3.6538e-02, 1.4619e-01, 2.0845e-01, 1.1493e-01, 6.8053e-02,\n",
       "          1.9061e-02, 1.9908e-01, 2.5854e-01, 5.1828e-02, 4.4614e-03, 5.1654e-02,\n",
       "          6.6198e-02, 1.4517e-02, 3.8072e-01, 6.3406e-01, 1.4859e-01, 9.8223e-01,\n",
       "          1.9690e-01, 3.9446e-02]]),\n",
       " torch.Size([404, 104]),\n",
       " torch.Size([102, 104]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the training and testing sets to torch tensors\n",
    "X_train_boston = torch.tensor(data=X_train_boston, dtype=torch.float32)\n",
    "X_test_boston = torch.tensor(data=X_test_boston, dtype=torch.float32)\n",
    "\n",
    "X_train_boston[:1], X_test_boston[:1], X_train_boston.shape, X_test_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[19.7000],\n",
       "         [23.1000],\n",
       "         [13.5000],\n",
       "         [21.2000],\n",
       "         [23.1000]]),\n",
       " tensor([[19.7000],\n",
       "         [23.1000],\n",
       "         [13.5000],\n",
       "         [21.2000],\n",
       "         [23.1000]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the labels to torch tensor\n",
    "y_train_boston = torch.tensor(data=np.array(object=y_train_boston).reshape(-1,1), dtype=torch.float32)\n",
    "y_test_boston = torch.tensor(data=np.array(object=y_test_boston).reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "# Sample teh data\n",
    "y_train_boston[:5], y_train_boston[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([404, 104]), torch.Size([404, 1]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the shape of the training and testing data\n",
    "X_train_boston.shape, y_train_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a training loop now that our model is defined.\n",
    "# Reusing the function created in the PyTorch classification\n",
    "#   17. Beginning Deep Learning, - Classification, Pytorch\n",
    "\n",
    "# In the following notebooks ...\n",
    "#   13. Beginning Deep Learning - Anomaly Detection with AutoEncoders, PyTroch\n",
    "#   15. Beginning Deep Learning, - Linear Regression, PyTorch\n",
    "# the training was all done outside of a function. Rather than rewriting the same code all the time\n",
    "# time to create a function\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "def torch_training(model=None, epochs=5, learning_rate=0.01, x_train=X_train_boston, \\\n",
    "                   y_train=y_train_boston, x_test=X_test_boston, y_test=y_test_boston):\n",
    "    ''' Performs training of the model '''\n",
    "    # Create to lists to save the training and test loss respectively \n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    # Setup the loss function\n",
    "    reg_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    # Setup the optimizer\n",
    "    reg_optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Clear the gradients\n",
    "        reg_optimizer.zero_grad()\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "\n",
    "        # Make predictions on the training data\n",
    "        train_preds = model(x_train)\n",
    "    \n",
    "        # Get the loss\n",
    "        train_loss = reg_loss_fn(train_preds, y_train)\n",
    "        training_loss.append(train_loss)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Upgrade the gradients\n",
    "        reg_optimizer.step()\n",
    "\n",
    "        # Evaluate the model at the same time\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            val_preds = model(x_test)\n",
    "\n",
    "            # Calculate the loss on the validation data\n",
    "            val_loss = reg_loss_fn(val_preds, y_test)\n",
    "            validation_loss.append(val_loss)\n",
    "\n",
    "        if epoch %10 == 0:\n",
    "            print(f'Epoch: {epoch} \\t training loss: {train_loss} \\t validation loss {val_loss}')\n",
    "    \n",
    "    return model, training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t training loss: 557.418212890625 \t validation loss 650.5715942382812\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "(model, train_loss, val_loss) = torch_training(model=torch_model, epochs=5, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157.1384"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How far off is the training score from the validation score\n",
    "np.var([np.mean(torch.tensor(train_loss).detach().numpy()),  np.mean(torch.tensor(val_loss).detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the manual seed\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# Training a torch model with dropout\n",
    "# Build a PyTorch model without dropout\n",
    "torch_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=X_train_boston.shape[1], out_features=256),\n",
    "    torch.nn.Dropout(p=0.5, inplace=True),\n",
    "    torch.nn.Linear(in_features=256, out_features=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a training loop now that our model is defined.\n",
    "# Reusing the function created in the PyTorch classification\n",
    "#   17. Beginning Deep Learning, - Classification, Pytorch\n",
    "\n",
    "# In the following notebooks ...\n",
    "#   13. Beginning Deep Learning - Anomaly Detection with AutoEncoders, PyTroch\n",
    "#   15. Beginning Deep Learning, - Linear Regression, PyTorch\n",
    "# the training was all done outside of a function. Rather than rewriting the same code all the time\n",
    "# time to create a function\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "def torch_training(model=None, epochs=5, learning_rate=0.01, x_train=X_train_boston, \\\n",
    "                   y_train=y_train_boston, x_test=X_test_boston, y_test=y_test_boston):\n",
    "    ''' Performs training of the model '''\n",
    "    # Create to lists to save the training and test loss respectively \n",
    "    training_loss, validation_loss = [], []\n",
    "\n",
    "    # Setup the loss function\n",
    "    reg_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    # Setup the optimizer\n",
    "    reg_optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Clear the gradients\n",
    "        reg_optimizer.zero_grad()\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "\n",
    "        # Make predictions on the training data\n",
    "        train_preds = model(x_train)\n",
    "    \n",
    "        # Get the loss\n",
    "        train_loss = reg_loss_fn(train_preds, y_train)\n",
    "        training_loss.append(train_loss)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Upgrade the gradients\n",
    "        reg_optimizer.step()\n",
    "\n",
    "        # Evaluate the model at the same time\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            val_preds = model(x_test)\n",
    "\n",
    "            # Calculate the loss on the validation data\n",
    "            val_loss = reg_loss_fn(val_preds, y_test)\n",
    "            validation_loss.append(val_loss)\n",
    "\n",
    "        if epoch %10 == 0:\n",
    "            print(f'Epoch: {epoch} \\t training loss: {train_loss} \\t validation loss {val_loss}')\n",
    "    \n",
    "    return model, training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t training loss: 555.8889770507812 \t validation loss 651.2589721679688\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "(model, train_loss, val_loss) = torch_training(model=torch_model, epochs=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1212.2544"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How far off is the training score from the validation score\n",
    "# A dropout of 0.5 did not improve this model\n",
    "np.var([np.mean(torch.tensor(train_loss).detach().numpy()),  np.mean(torch.tensor(val_loss).detach().numpy())])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional References and good reads: <br>\n",
    "https://www.dataquest.io/blog/regularization-in-machine-learning/ <br>\n",
    "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a <br>\n",
    "https://towardsdatascience.com/regularization-in-machine-learning-6fbc4417b1e5 <br>\n",
    "https://towardsdatascience.com/regularization-what-why-when-and-how-d4a329b6b27f <br>\n",
    "https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-regularization-techniques-in-machine-learning/ <br>\n",
    "https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf <br>\n",
    "https://www.analyticsvidhya.com/blog/2022/08/regularization-in-machine-learning/ <br>\n",
    "https://math.la.asu.edu/~samara/MLearn-lectures/regularization.html <br>\n",
    "https://medium.com/analytics-vidhya/regularization-in-machine-learning-7fb4e9d51f1d <br>\n",
    "https://www.youtube.com/watch?v=VqKq78PVO9g <br>\n",
    "https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf <br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout <br>\n",
    "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5 <br>\n",
    "https://towardsdatascience.com/types-of-regularization-in-machine-learning-eb5ce5f9bf50 <br>\n",
    "https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9 <br>\n",
    "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275 <br>\n",
    "https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf <br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcb100998b7ef434a33dbfac35f576e40d01f04c34d9ff6f6aad819b8a88c169"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
