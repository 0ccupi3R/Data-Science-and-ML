{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Nik Alleyne <br>\n",
    "Author Blog: **https://www.securitynik.com** <br>\n",
    "Author GitHub: **github.com/securitynik** <br>\n",
    "\n",
    "Author Books: [  <br>\n",
    "\n",
    "                \"https://www.amazon.ca/Learning-Practicing-Leveraging-Practical-Detection/dp/1731254458/\", \n",
    "                \n",
    "                \"https://www.amazon.ca/Learning-Practicing-Mastering-Network-Forensics/dp/1775383024/\" \n",
    "            ] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why this series?\n",
    "When teaching the SANS SEC595: Applied Data Science and Machine Learning for Cybersecurity Professionals \n",
    "**https://www.sans.org/cyber-security-courses/applied-data-science-machine-learning/** I am always asked,\n",
    "\"Will you be sharing your demo notebooks?\" or \"Can we get a copy of your demo notebooks?\" or ... well you get the point.\n",
    "My answer is always no. Not that I do not want to share, (sharing is caring :-D) , but the demo notebooks \n",
    "by themselves, would not make sense or add real value. Hence, this series! \n",
    "\n",
    "This is my supplemental work, similar to what I would do in the demos but with a lot more details and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Beginning Model TFServing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The series includes the following: <br>\n",
    "01 - Beginning Numpy <br>\n",
    "02 - Beginning Tensorflow  <br>\n",
    "03 - Beginning PyTorch <br>\n",
    "04 - Beginning Pandas <br>\n",
    "05 - Beginning Matplotlib <br>\n",
    "06 - Beginning Data Scaling <br>\n",
    "07 - Beginning Principal Component Analysis (PCA) <br>\n",
    "08 - Beginning Machine Learning Anomaly Detection - Isolation Forest and Local Outlier Factor <br>\n",
    "09 - Beginning Unsupervised Machine Learning - Clustering - K-means and DBSCAN <br>\n",
    "10 - Beginning Supervise Learning - Machine Learning - Logistic Regression, Decision Trees and Metrics <br>\n",
    "11 - Beginning Linear Regression - Machine Learning <br>\n",
    "12 - Beginning Deep Learning - Anomaly Detection with AutoEncoders, Tensorflow <br>\n",
    "13 - Beginning Deep Learning - Anomaly Detection with AutoEncoders, PyTroch <br>\n",
    "14 - Beginning Deep Learning - Linear Regression, Tensorflow <br>\n",
    "15 - Beginning Deep Learning - Linear Regression, PyTorch <br>\n",
    "16 - Beginning Deep Learning - Classification, Tensorflow <br>\n",
    "17 - Beginning Deep Learning - Classification, Pytorch <br>\n",
    "18 - Beginning Deep Learning - Classification - regression - MIMO - Functional API Tensorflow <br> \n",
    "19 - Beginning Deep Learning - Convolution Networks - Tensorflow <br>\n",
    "20 - Beginning Deep Learning - Convolution Networks - PyTorch <br>\n",
    "21 - Beginning Regularization - Early Stopping, Dropout, L2 (Ridge), L1 (Lasso) <br>\n",
    "22 - Beginning Model TFServing <br>\n",
    "\n",
    "But conn.log is not the only log file within Zeek. Let's build some models for DNS and HTTP logs. <br>\n",
    "I choose unsupervised, because there are no labels coming with these data. <br>\n",
    "\n",
    "23 - Continuing Anomaly Learning - Zeek DNS Log - Machine Learning <br>\n",
    "24 - Continuing Unsupervised Learning - Zeek HTTP Log - Machine Learning <br>\n",
    "\n",
    "This was a specific ask by someone in one of my class. <br>\n",
    "25 - Beginning - Reading Executables and Building a Neural Network to make predictions on suspicious vs suspicious  <br><br>\n",
    "\n",
    "With 25 notebooks in this series, it is quite possible there are things I could have or should have done differently.  <br>\n",
    "If you find any thing, you think fits those criteria, drop me a line. <br>\n",
    "\n",
    "If you find this series beneficial, I would greatly appreciate your feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we went through the notebooks above, we built a number of models. <br>\n",
    "Now that we have built those models, how do we use them?  <br>\n",
    "Well realistically, we already saw in almost all of the notebooks above,  <br>\n",
    "how we can save the model to disk and reload to make predictions. <br> \n",
    "\n",
    "Let's try something different. Time to take advantage of Tensorflow serving <br> \n",
    "\n",
    "I'm running Ubuntu on Windows Subsystem for Linux <br> <br>\n",
    "\n",
    "securitynik@SECURITYNIK-SURFACE:/Beginning_ML$ lsb_release --all <br> <br>\n",
    "No LSB modules are available. <br>\n",
    "Distributor ID: Ubuntu <br>\n",
    "Description:    Ubuntu 22.04.2 LTS <br>\n",
    "Release:        22.04 <br>\n",
    "Codename:       jammy <br> <br>\n",
    "\n",
    "My Docker version <br>\n",
    "securitynik@SECURITYNIK-SURFACE:/Beginning_ML$ docker --version <br>\n",
    "Docker version 23.0.2, build 569dd73 <br>\n",
    "\n",
    "### Install Tensorflow Serving on Ubuntu\n",
    "\n",
    "securitynik@SECURITYNIK-SURFACE:/Beginning_ML$ sudo docker pull tensorflow/serving <br>\n",
    "[sudo] password for securitynik: <br>\n",
    "Using default tag: latest <br>\n",
    "latest: Pulling from tensorflow/serving <br>\n",
    "ca1778b69356: Pull complete <br>\n",
    "bbb7c3e24bc6: Pull complete <br>\n",
    "c4f0a754ad10: Pull complete <br>\n",
    "74ed73ecd97e: Pull complete <br>\n",
    "a7597062a8f3: Pull complete <br>\n",
    "Digest: sha256:d3ddbce3278e428f2ea960ccece77ad58fa9f5e74d4c5320f031fdb85b4519c1 <br>\n",
    "Status: Downloaded newer image for tensorflow/serving:latest <br>\n",
    "docker.io/tensorflow/serving:latest <br>\n",
    "\n",
    "### With Tensorflow Serving installed, install the sample image <br>\n",
    "\n",
    "securitynik@SECURITYNIK-SURFACE:~$ git clone https://github.com/tensorflow/serving <br>\n",
    "Cloning into 'serving'... <br>\n",
    "remote: Enumerating objects: 32871, done. <br>\n",
    "remote: Counting objects: 100% (142/142), done. <br>\n",
    "remote: Compressing objects: 100% (82/82), done. <br>\n",
    "remote: Total 32871 (delta 60), reused 120 (delta 53), pack-reused 32729 <br>\n",
    "Receiving objects: 100% (32871/32871), 18.07 MiB | 15.75 MiB/s, done. <br>\n",
    "Resolving deltas: 100% (26363/26363), done. <br>\n",
    "\n",
    "### Load up docker with the test image <br>\n",
    "\n",
    "securitynik@SECURITYNIK-SURFACE:~$  docker run --tty --interactive  --rm --publish 8501:8501 --volume \"./serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu:/models/half_plus_two\" --env MODEL_NAME=half_plus_two tensorflow/serving <br>\n",
    "...\n",
    "2023-05-05 03:07:36.277473: I tensorflow_serving/core/loader_harness.cc:95] Successfully loaded servable version {name: half_plus_two version: 123} <br>\n",
    "... <br>\n",
    "2023-05-05 03:07:36.281594: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
    "[warn] getaddrinfo: address family for nodename not supported <br>\n",
    "2023-05-05 03:07:36.287003: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...\n",
    "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ... <br>\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/tfx/serving/docker <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a prediction using curl <br>\n",
    "securitynik@SECURITYNIK-SURFACE:~$  **curl -data '{\"instances\" : [1.0, 2.0, 5.0]}' --request POST http://172.17.0.1:8501/v1/models/half_plus_two:predict** <br>\n",
    "{ <br>\n",
    "    \"predictions\": [2.5, 3.0, 4.5 <br>\n",
    "    ] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a prediction with Powershell <br>\n",
    "PS C:\\Users\\SecurityNik> **Invoke-WebRequest -Uri \"http://172.19.64.79:8501/v1/models/half_plus_two:predict\" -Method POST -Body '{\"instances\" : [1.0, 2.0, 5.0]}' | Select-Object -ExpandProperty Content** <br>\n",
    "{ <br>\n",
    "    \"predictions\": [2.5, 3.0, 4.5 <br>\n",
    "    ] <br>\n",
    "} <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are aware how to use Tensorflow Serving, time to load our own model rather than the test. <br>\n",
    "Let's use the autoencoder which was built in notebook: <br>\n",
    "    12. Beginning Deep Learning - Anomaly Detection with AutoEncoders, Tensorflow <br>\n",
    "\n",
    "Interestingly my current directory structure for the model we will use, looks like.  <br>\n",
    " <br>\n",
    "├───TF_AUTOENCODER <br>\n",
    "│   └───tf_autoencoder.tf <br>\n",
    "│       ├───assets <br>\n",
    "│       └───variables <br>\n",
    " <br>\n",
    "However, we need to add some version information, so create a directory \"1\" under \"tf_autoencoder.tf\" and move the files into it <br>\n",
    " <br>\n",
    "├───TF_AUTOENCODER <br>\n",
    "│   └───tf_autoencoder.tf <br>\n",
    "│       └───1 <br>\n",
    "│           ├───assets <br>\n",
    "│           └───variables <br>\n",
    "\n",
    "Loading the autoencoder model <br>\n",
    " <br>\n",
    "securitynik@SECURITYNIK-SURFACE:~$ **docker run --tty --interactive  --rm --publish 8501:8501 --volume \"/mnt/d/ML/BeginningML/SAVED_MODELS/TF_AUTOENCODER/tf_autoencoder.tf/:/models/tf_autoencoder\" --env MODEL_NAME=tf_autoencoder tensorflow/serving** <br> <br>\n",
    "...\n",
    "...\n",
    "2023-05-28 17:28:20.737027: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /models/tf_autoencoder/1 <br>\n",
    "2023-05-28 17:28:20.754434: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 217682 microseconds. <br>\n",
    "...\n",
    "2023-05-28 17:28:20.983646: I tensorflow_serving/core/loader_harness.cc:95] Successfully loaded servable version {name: tf_autoencoder version: 1} <br>\n",
    "2023-05-28 17:28:20.988505: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models <br>\n",
    "2023-05-28 17:28:20.988650: I tensorflow_serving/model_servers/server.cc:118] Using InsecureServerCredentials <br>\n",
    "2023-05-28 17:28:20.988678: I tensorflow_serving/model_servers/server.cc:383] Profiler service is enabled <br>\n",
    "2023-05-28 17:28:20.989675: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
    "[warn] getaddrinfo: address family for nodename not supported <br>\n",
    "2023-05-28 17:28:20.995707: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...\n",
    "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ... <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the service successfully loaded, we can now go about sending data to our model.  <br>\n",
    "\n",
    "Make a prediction with curl <br>\n",
    "securitynik@SECURITYNIK-SURFACE:~$ **curl --data '{\"instances\" : [[141.0, 356138566.0, 11037090.0, 60.0, 3026679.0, 33.0, 982584.0]]}' --request POST http://172.17.0.1:8501/v1/models/tf_autoencoder:predict** <br>\n",
    "{ <br>\n",
    "    \"**predictions\": [[3.53547932e-07, 5.11308649e-08, 1.05425784e-08, 3.68306837e-06, 3.3058825e-07, 1.75808657e-06, 1.29566089e-07]** <br>\n",
    "    ] <br>\n",
    "} <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction with Powershell <br>\n",
    " <br>\n",
    "PS D:\\> **Invoke-WebRequest -Uri \"http://172.19.64.79:8501/v1/models/tf_autoencoder:predict\" -Method POST -Body '{\"instances\" :  [[141.0, 356138566.0, 11037090.0, 60.0, 3026679.0, 33.0, 982584.0]]}' | Select-Object -ExpandProperty Content**   <br>                                                                                                                                             <br>{\n",
    "    \"**predictions\": [[3.53547932e-07, 5.11308649e-08, 1.05425784e-08, 3.68306837e-06, 3.3058825e-07, 1.75808657e-06, 1.29566089e-07]** <br>\n",
    "    ] <br>\n",
    "} <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we were able to make predictions, this is not exactly what we wanted. <br>\n",
    "We still need to calculate the Mean Absolute Error (MAE) for these predictions. <br>\n",
    "Basically, the prediction alone above is not good enough for us. <br>\n",
    "Let's address this shortcoming! <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4100000e+02, 3.5613856e+08, 1.1037090e+07, 6.0000000e+01,\n",
       "        3.0266790e+06, 3.3000000e+01, 9.8258400e+05]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing our sample as a variable\n",
    "new_sample = np.array([141., 356138566,\t11037090, 60, 3026679, 33, 982584], dtype=np.float32, ndmin=2)\n",
    "new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before making a prediction we need to scale the data as was done during the model training in \n",
    "#   12 - Beginning Deep Learning - Anomaly Detection with AutoEncoders, Tensorflow\n",
    "#   13 - Beginning Deep Learning - Anomaly Detection with AutoEncoders, PyTroch <br>\n",
    "#\n",
    "#   https://machinelearningmastery.com/how-to-save-and-load-models-and-data-preparation-in-scikit-learn-for-later-use/\n",
    "#   https://stackabuse.com/bytes/how-to-save-and-load-fit-scikit-learn-scalers/\n",
    "# Load the scaler which was saved in the above notebook\n",
    "min_max_scaler = pickle.load(file=open('tf_min_max_scaler.pkl', mode='rb'))\n",
    "min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.2334208e-03, 6.1739308e-01, 5.2606082e-03, 2.1844395e-04,\n",
       "        1.3436210e-02, 5.7754605e-05, 1.6974034e-03]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With the scaler loaded, time to transform our sample\n",
    "# Remember, we need to preprocess the test or any prediction data, the same way as we did with the training data\n",
    "new_sample_scaled = min_max_scaler.transform(new_sample)\n",
    "new_sample_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"instances\": [[0.004233420826494694, 0.6173930764198303, 0.005260608159005642, 0.00021844395087100565, 0.013436210341751575, 5.775460522272624e-05, 0.0016974033787846565]]}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the data in json format to be sent to the model for prediction\n",
    "data = json.dumps({'instances' : new_sample_scaled.tolist() })\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now access our model which is being served via TFServing\n",
    "# Previously, we used curl as well as Invoke-WebReqest\n",
    "# Time to do things differently\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content-type': 'application/json', 'user-agent': 'securitynik'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a header content-type as Javascript Object Notation (JSON)\n",
    "headers = {'content-type' : 'application/json', 'user-agent' : 'securitynik'}\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.58950103e-07,\n",
       "  5.13086675e-08,\n",
       "  1.05235536e-08,\n",
       "  3.69800182e-06,\n",
       "  3.30785667e-07,\n",
       "  1.75776813e-06,\n",
       "  1.29550529e-07]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a request\n",
    "#   https://keras.io/examples/keras_recipes/tf_serving/\n",
    "#   \n",
    "json_response = requests.post(url='http://172.19.64.79:8501/v1/models/tf_autoencoder:predict', \\\n",
    "                              headers=headers, data=data)\n",
    "\n",
    "# Store the predictions\n",
    "initial_predictions = json_response.json()['predictions']\n",
    "\n",
    "# What does the predictions look like\n",
    "initial_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401097.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a threshold which was set during development of the model\n",
    "#   12. Beginning Deep Learning - Anomaly Detection with AutoEncoders, Tensorflow\n",
    "THRESHOLD = 2401097.2\n",
    "THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([53026452.], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that's progress! With the predictions in place and the threshold defined, let's move forward\n",
    "suspicious_or_not = tf.keras.losses.mae(y_true=new_sample, y_pred=initial_predictions)\n",
    "suspicious_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datetime library\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-28 23:00:41.939407 - [!] ALERT ** SUSPICIOUS ACTIVITY ** Zeek conn.log entry Threshold Exceeded'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write some code to trigger an alert when a sample's reconstruction error passes our threshold\n",
    "f'{datetime.now()} - [!] ALERT ** SUSPICIOUS ACTIVITY ** Zeek conn.log entry Threshold Exceeded' \\\n",
    "    if suspicious_or_not > THRESHOLD  else \"[**] Normal Traffic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we got the sample being reported as suspicious \n",
    "# Time to move on!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional references: <br>\n",
    "https://neptune.ai/blog/how-to-serve-machine-learning-models-with-tensorflow-serving-and-docker <br>\n",
    "https://grpc.io/ <br>\n",
    "https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them <br>\n",
    "https://www.tensorflow.org/tfx/serving/serving_config <br>\n",
    "https://neptune.ai/blog/how-to-serve-machine-learning-models-with-tensorflow-serving-and-docker <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
